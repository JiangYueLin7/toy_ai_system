import cv2
import numpy as np
import os
import sys
import hashlib
from tqdm import tqdm
from skimage.metrics import structural_similarity as ssim
from sklearn.metrics.pairwise import cosine_similarity
from concurrent.futures import ThreadPoolExecutor
import tensorflow as tf

def safe_imread(img_path):
    """å¢å¼ºå‹å®‰å…¨è¯»å–ï¼Œæ”¯æŒä¸­æ–‡/ç‰¹æ®Šå­—ç¬¦è·¯å¾„"""
    try:
        # è§„èŒƒåŒ–è·¯å¾„æ ¼å¼
        img_path = os.path.normpath(img_path).replace('\\', '/')
        
        # å°è¯•OpenCVè¯»å–
        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        if img is not None:
            return img
        
        # OpenCVå¤±è´¥æ—¶ä½¿ç”¨Pillowè¯»å–
        from PIL import Image
        img_pil = Image.open(img_path).convert('L')
        return np.array(img_pil)
    except Exception as e:
        print(f"ğŸ”´ è¯»å–å¤±è´¥: {img_path} - {str(e)}")
        return None

def validate_file_integrity(file_path):
    """æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§ï¼ˆMD5æ ¡éªŒï¼‰"""
    hash_md5 = hashlib.md5()
    try:
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    except Exception as e:
        print(f"ğŸ”´ æ–‡ä»¶æ ¡éªŒå¤±è´¥: {file_path} - {str(e)}")
        return None

def check_path_length(path, max_len=260):
    """æ£€æŸ¥è·¯å¾„é•¿åº¦ï¼ˆWindowsé™åˆ¶260å­—ç¬¦ï¼‰"""
    if len(path) > max_len:
        print(f"âš ï¸ è·¯å¾„è¿‡é•¿: {path} ({len(path)} > {max_len})")
        return False
    return True

def dhash(image_path, hash_size=16):
    """æ”¹è¿›çš„å·®å€¼å“ˆå¸Œç®—æ³•"""
    image = safe_imread(image_path)
    if image is None:
        return None
    
    try:
        resized = cv2.resize(image, (hash_size + 1, hash_size))
        diff = resized[:, 1:] > resized[:, :-1]
        # return sum([2 â€‹**â€‹ i for i, v in enumerate(diff.flatten()) if v])
        return sum([2 ** i for i, v in enumerate(diff.flatten()) if v])
    except Exception as e:
        print(f"ğŸ”´ å“ˆå¸Œè®¡ç®—å¤±è´¥: {image_path} - {str(e)}")
        return None

def feature_extraction(image_path):
    """ä½¿ç”¨ResNet50æå–æ·±åº¦ç‰¹å¾"""
    try:
        model = tf.keras.applications.ResNet50(
            weights='imagenet',
            include_top=False,
            pooling='avg'
        )
        img = tf.keras.preprocessing.image.load_img(
            image_path,
            target_size=(224, 224)
        )
        img_array = tf.keras.preprocessing.image.img_to_array(img)
        img_array = np.expand_dims(img_array, axis=0)
        return model.predict(img_array).flatten()
    except Exception as e:
        print(f"ğŸ”´ ç‰¹å¾æå–å¤±è´¥: {image_path} - {str(e)}")
        return None

def get_leaf_directories(root_dir):
    """è·å–æ‰€æœ‰æœ€åº•å±‚å­ç›®å½•"""
    leaf_dirs = []
    for dirpath, dirnames, _ in os.walk(root_dir):
        if not dirnames:
            leaf_dirs.append(dirpath)
    return leaf_dirs

def deduplicate_images(root_dir, output_dir):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # è·å–æ‰€æœ‰æœ€åº•å±‚å­ç›®å½•
    leaf_dirs = get_leaf_directories(root_dir)
    if not leaf_dirs:
        print("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆå­ç›®å½•")
        return
    
    # æ”¶é›†æ‰€æœ‰æœ‰æ•ˆå›¾ç‰‡
    all_images = []
    for leaf_dir in leaf_dirs:
        for entry in os.scandir(leaf_dir):
            if entry.is_file() and entry.name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):
                img_path = entry.path
                if check_path_length(img_path) and validate_file_integrity(img_path):
                    all_images.append(img_path)
    
    if not all_images:
        print("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆå›¾ç‰‡")
        return
    
    # å“ˆå¸Œå»é‡ï¼ˆå…¨å±€ï¼‰
    print("1/3 å“ˆå¸Œå»é‡ä¸­...")
    hash_dict = {}
    for img_path in tqdm(all_images, desc="è®¡ç®—å“ˆå¸Œå€¼"):
        img_hash = dhash(img_path)
        if img_hash is not None:
            hash_dict[img_hash] = img_path
    
    # ç‰¹å¾å»é‡ï¼ˆæŒ‰å­ç›®å½•åˆ†ç»„ï¼‰
    print("\n2/3 ç‰¹å¾å»é‡ä¸­...")
    group_features = {}
    for img_path in tqdm(hash_dict.values(), desc="æå–ç‰¹å¾"):
        # ç¡®å®šæ‰€å±å­ç›®å½•
        for leaf_dir in leaf_dirs:
            if img_path.startswith(leaf_dir):
                group_id = leaf_dir
                break
                
        feature = feature_extraction(img_path)
        if feature is None:
            continue
            
        if group_id not in group_features:
            group_features[group_id] = {
                'features': [],
                'paths': []
            }
            
        group_features[group_id]['features'].append(feature)
        group_features[group_id]['paths'].append(img_path)
    
    # ä¿å­˜å»é‡ç»“æœ
    print("\n3/3 ä¿å­˜ç»“æœä¸­...")
    for group_id, group_data in group_features.items():
        features = np.array(group_data['features'])
        paths = group_data['paths']
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        sim_matrix = cosine_similarity(features)
        np.fill_diagonal(sim_matrix, 0)  # å¿½ç•¥è‡ªèº«
        
        # ä¿ç•™ç›¸ä¼¼åº¦æœ€ä½çš„å›¾ç‰‡
        keep_indices = []
        while len(sim_matrix) > 0:
            min_sim = np.min(sim_matrix)
            if min_sim >= 0.9:
                break  # åœæ­¢æ¡ä»¶
            
            min_index = np.unravel_index(np.argmin(sim_matrix), sim_matrix.shape)
            keep_indices.append(min_index[0])
            
            # ç§»é™¤å·²é€‰ä¸­å›¾ç‰‡
            sim_matrix = np.delete(sim_matrix, min_index[0], axis=0)
            sim_matrix = np.delete(sim_matrix, min_index[0], axis=1)
        
        # ä¿å­˜ä¿ç•™çš„å›¾ç‰‡
        for idx in keep_indices:
            src = paths[idx]
            dst = os.path.join(output_dir, os.path.basename(src))
            os.makedirs(os.path.dirname(dst), exist_ok=True)
            os.rename(src, dst)
            print(f"âœ… ä¿ç•™: {src} â†’ {dst}")

if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("ç”¨æ³•: python deduplicate.py <è¾“å…¥æ ¹ç›®å½•> <è¾“å‡ºç›®å½•>")
        sys.exit(1)
    
    input_root = sys.argv[1]
    output_root = sys.argv[2]
    
    deduplicate_images(input_root, output_root)
# check_train.py
import sys
import random
import numpy as np
from common import create_data_generator, build_base_model, create_minimal_model
import os
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

def validate_pipeline(data_path, img_size=450, batch_size=8, epochs=1):
    """ç«¯åˆ°ç«¯éªŒè¯ç®¡é“"""
    
    # 1. æ•°æ®éªŒè¯
    try:
        data_gen = create_data_generator(
            data_path=data_path,
            target_size=img_size,
            batch_size=batch_size,
            validation_split=0.0  # ä¸éœ€è¦éªŒè¯é›†
        )
        
        # éªŒè¯è‡³å°‘å­˜åœ¨10å¼ å›¾ç‰‡
        all_samples = []
        for _, _, files in os.walk(data_path):
            all_samples.extend(files)
        if len(all_samples) < 10:
            raise FileNotFoundError(f"æ•°æ®é›†éœ€è‡³å°‘åŒ…å«10å¼ å›¾ç‰‡ï¼ˆå½“å‰{len(all_samples)}å¼ ï¼‰")
            
    except Exception as e:
        print(f"ğŸ”´ æ•°æ®éªŒè¯å¤±è´¥: {str(e)}")
        return False
    
    # 2. æ¨¡å‹éªŒè¯
    try:
        # åˆ›å»ºç®€åŒ–ç‰ˆæ¨¡å‹
        base_model = build_base_model(input_shape=(img_size, img_size, 3))
        minimal_model = create_minimal_model(base_model, num_classes=2)  # å‡è®¾2ç±»
        
        # ç¼–è¯‘æ¨¡å‹ï¼ˆä½¿ç”¨æç®€é…ç½®ï¼‰
        minimal_model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
    except Exception as e:
        print(f"ğŸ”´ æ¨¡å‹æ„å»ºå¤±è´¥: {str(e)}")
        return False
    
    # 3. è®­ç»ƒéªŒè¯
    try:
        # éšæœºé‡‡æ ·10å¼ å›¾ç‰‡
        sample_files = random.sample(all_samples, 10)
        sample_x = []
        sample_y = []
        for file in sample_files:
            img = tf.keras.preprocessing.image.load_img(
                file,
                target_size=(img_size, img_size)
            )
            img_array = tf.keras.preprocessing.image.img_to_array(img)
            sample_x.append(img_array)
            # å‡è®¾æ–‡ä»¶ååŒ…å«ç±»åˆ«ä¿¡æ¯ï¼ˆéœ€æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰
            label = int(file.split('/')[-2])
            sample_y.append([1 if i == label else 0 for i in range(2)])
            
        sample_x = np.array(sample_x)
        sample_y = np.array(sample_y)
        
        # æ‰§è¡Œå¿«é€Ÿè®­ç»ƒ
        history = minimal_model.fit(
            x=sample_x,
            y=sample_y,
            epochs=epochs,
            verbose=1,
            callbacks=[
                EarlyStopping(patience=1),
                ReduceLROnPlateau(patience=1)
            ]
        )
        
        # éªŒè¯æŸå¤±æ”¶æ•›
        if np.isnan(history.history['loss'][0]):
            raise ValueError("è®­ç»ƒæŸå¤±å‡ºç°NaN")
            
    except Exception as e:
        print(f"ğŸ”´ è®­ç»ƒéªŒè¯å¤±è´¥: {str(e)}")
        return False
    
    print("\nâœ… éªŒè¯é€šè¿‡ï¼å¯ä»¥å®‰å…¨æ‰§è¡Œå®Œæ•´è®­ç»ƒ")
    return True

if __name__ == "__main__":
    data_path = sys.argv[1] if len(sys.argv) > 1 else "train_data"
    success = validate_pipeline(data_path)
    exit(0 if success else 1)